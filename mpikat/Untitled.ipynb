{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def ip_range_from_stream(stream):\n",
    "    stream = stream.lstrip(\"spead://\")\n",
    "    ip_range, port = stream.split(\":\")\n",
    "    port = int(port)\n",
    "    try:\n",
    "        base_ip, ip_count = ip_range.split(\"+\")\n",
    "        ip_count = int(ip_count)\n",
    "    except ValueError:\n",
    "        base_ip, ip_count = ip_range, 1\n",
    "    return [ipaddress.ip_address(unicode(base_ip))+i for i in range(ip_count)], port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips,_ = ip_range_from_stream(\"spead://123.1.1.1+10:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IPv4Address(u'123.1.1.1'),\n",
       " IPv4Address(u'123.1.1.2'),\n",
       " IPv4Address(u'123.1.1.3'),\n",
       " IPv4Address(u'123.1.1.4'),\n",
       " IPv4Address(u'123.1.1.5'),\n",
       " IPv4Address(u'123.1.1.6'),\n",
       " IPv4Address(u'123.1.1.7'),\n",
       " IPv4Address(u'123.1.1.8'),\n",
       " IPv4Address(u'123.1.1.9'),\n",
       " IPv4Address(u'123.1.1.10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ips.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_range_from_stream(stream):\n",
    "    stream = stream.lstrip(\"spead://\")\n",
    "    ip_range, port = stream.split(\":\")\n",
    "    port = int(port)\n",
    "    try:\n",
    "        base_ip, ip_count = ip_range.split(\"+\")\n",
    "        ip_count = int(ip_count)\n",
    "    except ValueError:\n",
    "        base_ip, ip_count = ip_range, 1\n",
    "    return ContiguousIpRange(base_ip, port, ip_count)\n",
    "\n",
    "class IpRangeAllocationError(Exception):\n",
    "    pass\n",
    "\n",
    "class ContiguousIpRange(object):\n",
    "    def __init__(self, base_ip, port, count):\n",
    "        self._base_ip = ipaddress.ip_address(unicode(base_ip))\n",
    "        self._ips = [self._base_ip+ii for ii in range(count)]\n",
    "        self._port = port\n",
    "        self._count = count\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return self._count\n",
    "\n",
    "    @property\n",
    "    def port(self):\n",
    "        return self._port\n",
    "\n",
    "    @property\n",
    "    def base_ip(self):\n",
    "        return self._base_ip\n",
    "\n",
    "    def index(self, ip):\n",
    "        return self._ips.index(ip)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.format_katcp())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._ips.__iter__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<{} {}>\".format(self.__class__.__name__, self.format_katcp())\n",
    "\n",
    "    def format_katcp(self):\n",
    "        return \"spead://{}+{}:{}\".format(str(self._base_ip), self._count, self._port)\n",
    "\n",
    "\n",
    "class IpRangeManager(object):\n",
    "    def __init__(self, ip_range):\n",
    "        self._ip_range = ip_range\n",
    "        self._allocated = [False for _ in ip_range]\n",
    "        self._allocated_ranges = set()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<{} {}>\".format(self.__class__.__name__, self._ip_range.format_katcp())\n",
    "\n",
    "    def _free_ranges(self):\n",
    "        state_ranges = {True:[], False:[]}\n",
    "        def find_state_range(idx, state):\n",
    "            start_idx = idx\n",
    "            while idx < len(self._allocated):\n",
    "                if self._allocated[idx] == state:\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    state_ranges[state].append((start_idx, idx-start_idx))\n",
    "                    return find_state_range(idx, not state)\n",
    "            else:\n",
    "                state_ranges[state].append((start_idx, idx-start_idx))\n",
    "        find_state_range(0, self._allocated[0])\n",
    "        return state_ranges[False]\n",
    "\n",
    "    def allocate(self, n):\n",
    "        ranges = self._free_ranges()\n",
    "        best_fit = None\n",
    "        for start,span in ranges:\n",
    "            if span<n:\n",
    "                continue\n",
    "            elif best_fit is None:\n",
    "                best_fit = (start, span)\n",
    "            elif (span-n) < (best_fit[1]-n):\n",
    "                best_fit = (start, span)\n",
    "        if best_fit is None:\n",
    "            raise IpRangeAllocationError(\"Could not allocate contiguous range of {} addresses\".format(n))\n",
    "        else:\n",
    "            start,span = best_fit\n",
    "            for ii in range(n):\n",
    "                offset = start+ii\n",
    "                self._allocated[offset] = True\n",
    "            allocated_range = ContiguousIpRange(str(self._ip_range.base_ip + start), self._ip_range.port, n)\n",
    "            self._allocated_ranges.add(allocated_range)\n",
    "            return allocated_range\n",
    "\n",
    "    def free(self, ip_range):\n",
    "        self._allocated_ranges.remove(ip_range)\n",
    "        for ip in ip_range:\n",
    "            self._allocated[self._ip_range.index(ip)] = False\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IpRangeManager spead://239.1.1.150+128:7147>\n"
     ]
    }
   ],
   "source": [
    "x = IpRangeManager(ip_range_from_stream('spead://239.1.1.150+128:7147'))\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x._allocated[5:9] = [True for _ in range(4)]\n",
    "x._allocated[10:11] = [True for _ in range(1)]\n",
    "x._allocated[56:77] = [True for _ in range(77-56)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (9, 1), (11, 45), (77, 51)]\n",
      "set([<ContiguousIpRange spead://239.1.1.161+40:7147>])\n",
      "<IpRangeManager spead://239.1.1.150+128:7147>\n"
     ]
    }
   ],
   "source": [
    "x = IpRangeManager(ip_range_from_stream('spead://239.1.1.150+128:7147'))\n",
    "x._allocated[5:9] = [True for _ in range(4)]\n",
    "x._allocated[10:11] = [True for _ in range(1)]\n",
    "x._allocated[56:77] = [True for _ in range(77-56)]\n",
    "print x._free_ranges()\n",
    "a = x.allocate(40)\n",
    "print x._allocated_ranges\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spead://239.1.1.161+40:7147\n"
     ]
    }
   ],
   "source": [
    "print a.format_katcp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (9, 1), (51, 5), (77, 51)]\n"
     ]
    }
   ],
   "source": [
    "print x._free_ranges()\n",
    "x.free(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (9, 1), (11, 45), (77, 51)]\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "print x._free_ranges()\n",
    "print x._allocated_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "x.index(2)\n",
    "a = x.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__length_hint__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'next']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.n_servers = 64\n",
    "        self.n_mcast_groups = 128\n",
    "        \n",
    "    def _sanitize_sb_configuration(self,  tscrunch, fscrunch, desired_nbeams, beam_granularity=None):\n",
    "\n",
    "        # What are the key constraints:\n",
    "        # 1. The data rate per multicast group\n",
    "        # 2. The aggregate data rate out of the instrument (not as important)\n",
    "        # 3. The processing limitations (has to be determined empirically)\n",
    "        # 4. The number of multicast groups available\n",
    "        # 5. The possible numbers of beams per multicast group (such that TUSE can receive N per node)\n",
    "        # 6. Need to use at least 16 multicast groups\n",
    "        # 7. Should have even flow across multicast groups, so same number of beams in each\n",
    "        # 8. Multicast groups should be contiguous\n",
    "\n",
    "\n",
    "        # Constants for data rates and bandwidths\n",
    "        # these are hardcoded here for the moment but ultimately\n",
    "        # they should be moved to a higher level or even dynamically\n",
    "        # specified\n",
    "        MAX_RATE_PER_MCAST = 6.8e9 # bits/s\n",
    "        MAX_RATE_PER_SERVER = 4.375e9 # bits/s, equivalent to 280 Gb/s over 64 (virtual) nodes\n",
    "        MAX_OUTPUT_RATE = 280.0e9 # bits/s\n",
    "        BANDWIDTH = 856e6 # MHz\n",
    "\n",
    "        # Calculate the data rate for each beam assuming 8-bit packing and\n",
    "        # no metadata overheads\n",
    "        data_rate_per_beam = BANDWIDTH / tscrunch / fscrunch * 8 # bits/s\n",
    "        log.debug(\"Data rate per coherent beam: {} Gb/s\".format(data_rate_per_beam/1e9))\n",
    "\n",
    "        if data_rate_per_beam > MAX_RATE_PER_MCAST:\n",
    "            raise Exception(\"Data rate per beam is greater than the data rate per multicast group\")\n",
    "            \n",
    "        if data_rate_per_beam * desired_nbeams > MAX_OUTPUT_RATE:\n",
    "            desired_nbeams = MAX_OUTPUT_RATE // data_rate_per_beam\n",
    "            log.warning(\"Aggregate data rate larger than supported, reducing nbeams to {}\".format(desired_nbeams))\n",
    "             \n",
    "        # Calculate the maximum number of beams that will fit in one multicast\n",
    "        # group assuming. Each multicast group must be receivable on a 10 GbE\n",
    "        # connection so the max rate must be < 8 Gb/s\n",
    "        max_beams_per_mcast = MAX_RATE_PER_MCAST // data_rate_per_beam\n",
    "        log.debug(\"Maximum number of beams per multicast group: {}\".format(int(max_beams_per_mcast)))\n",
    "\n",
    "        # For instuments such as TUSE, they require a fixed number of beams per node. For their\n",
    "        # case we assume that they will only acquire one multicast group per node and as such\n",
    "        # the minimum number of beams per multicast group should be whatever TUSE requires.\n",
    "        # Multicast groups can contain more beams than this but only in integer multiples of\n",
    "        # the minimum\n",
    "        if beam_granularity:\n",
    "            if max_beams_per_mcast < beam_granularity:\n",
    "                log.warning(\"Cannot fit {} beams into one multicast group, updating number of beams per multicast group to {}\".format(\n",
    "                    beam_granularity, max_beams_per_mcast))\n",
    "                while np.modf(beam_granularity/max_beams_per_mcast)[0] != 0.0:\n",
    "                    max_beams_per_mcast -= 1\n",
    "                beam_granularity = max_beams_per_mcast\n",
    "            beams_per_mcast = beam_granularity * (max_beams_per_mcast // beam_granularity)\n",
    "            log.debug(\"Number of beams per multicast group, accounting for granularity: {}\".format(int(beams_per_mcast)))\n",
    "        else:\n",
    "            beams_per_mcast = max_beams_per_mcast\n",
    "\n",
    "        # Calculate the total number of beams that could be produced assuming the only\n",
    "        # rate limit was that limit per multicast groups\n",
    "        max_beams = self.n_mcast_groups * beams_per_mcast\n",
    "        log.debug(\"Maximum possible beams (assuming on multicast group rate limit): {}\".format(max_beams))\n",
    "\n",
    "        if desired_nbeams > max_beams:\n",
    "            log.warning(\"Requested number of beams is greater than theoretical maximum, \"\n",
    "                \"updating setting the number of beams of beams to {}\".format(max_beams))\n",
    "            desired_nbeams = max_beams\n",
    "\n",
    "        # Calculate the total number of multicast groups that are required to satisfy\n",
    "        # the requested number of beams\n",
    "        num_mcast_groups_required = round(desired_nbeams / beams_per_mcast + 0.5)\n",
    "        log.debug(\"Number of multicast groups required for {} beams: {}\".format(desired_nbeams, num_mcast_groups_required))\n",
    "        actual_nbeams = num_mcast_groups_required * beams_per_mcast\n",
    "        nmcast_groups = num_mcast_groups_required\n",
    "\n",
    "        # Now we need to check the server rate limits\n",
    "        if (actual_nbeams * data_rate_per_beam)/self.n_servers > MAX_RATE_PER_SERVER:\n",
    "            log.warning(\"Number of beams limited by output data rate per server\")\n",
    "        actual_nbeams = MAX_RATE_PER_SERVER*self.n_servers // data_rate_per_beam\n",
    "        log.info(\"Number of beams that can be generated: {}\".format(actual_nbeams))\n",
    "        return actual_nbeams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Data rate per coherent beam: 1.3696 Gb/s\n",
      "DEBUG:root:Maximum number of beams per multicast group: 4\n",
      "WARNING:root:Cannot fit 6 beams into one multicast group, updating number of beams per multicast group to 4.0\n",
      "DEBUG:root:Number of beams per multicast group, accounting for granularity: 3\n",
      "DEBUG:root:Maximum possible beams (assuming on multicast group rate limit): 384.0\n",
      "DEBUG:root:Number of multicast groups required for 16 beams: 6.0\n",
      "INFO:root:Number of beams that can be generated: 204.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Config()\n",
    "x._sanitize_sb_configuration(5,1,16,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticastGroupConfig(object):\n",
    "    def __init__(self, data_rate_per_beam, granularity):\n",
    "        self.data_rate_per_beam = data_rate_per_beam\n",
    "        self.granularity = granularity\n",
    "        self.group_count = 16\n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_group(self):\n",
    "        prior_nbeams = self.group_count * self.beam_count\n",
    "        self.group_count += 1\n",
    "        self.beam_count = round(prior_nbeams / self.group_count + 0.5)\n",
    "    \n",
    "    def nbeams(self):\n",
    "        return self.group_count * self.beam_count\n",
    "    \n",
    "    def add_beam(self):\n",
    "        self.beam_count+=1\n",
    "        \n",
    "    def remove_beam(self):\n",
    "        self.beam_count-=1\n",
    "        \n",
    "    def matches_granularity(self, granularity):\n",
    "        if self.beam_count % granularity == 0:\n",
    "            return True\n",
    "        elif granularity % self.beam_count == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "def test():\n",
    "    group = MulticastGroup()\n",
    "        \n",
    "class MulticastConfiguration(object):\n",
    "    def __init__(self, nservers, max_groups):\n",
    "        self.nservers = nservers\n",
    "        self.max_groups = max_groups\n",
    "        self.base_groups = [[] for _ in range(16)]\n",
    "        \n",
    "    def config(self, tscrunch, fscrunch, desired_nbeams, beam_granularity):\n",
    "        MAX_RATE_PER_MCAST = 6.8e9 # bits/s\n",
    "        MAX_RATE_PER_SERVER = 4.375e9 # bits/s, equivalent to 280 Gb/s over 64 (virtual) nodes\n",
    "        BANDWIDTH = 856e6 # MHz\n",
    "\n",
    "        # Calculate the data rate for each beam assuming 8-bit packing and\n",
    "        # no metadata overheads\n",
    "        data_rate_per_beam = BANDWIDTH / tscrunch / fscrunch * 8 # bits/s\n",
    "        log.debug(\"Data rate per coherent beam: {} Gb/s\".format(data_rate_per_beam/1e9))\n",
    "        # Calculate the maximum number of beams that will fit in one multicast\n",
    "        # group assuming. Each multicast group must be receivable on a 10 GbE\n",
    "        # connection so the max rate must be < 8 Gb/s\n",
    "        max_beams_per_mcast = MAX_RATE_PER_MCAST // data_rate_per_beam\n",
    "        log.debug(\"Maximum number of beams per multicast group: {}\".format(int(max_beams_per_mcast)))\n",
    "\n",
    "        if max_beams_per_mcast == 0:\n",
    "            raise Exception(\"Data rate per beam is greater than the data rate per multicast group\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
